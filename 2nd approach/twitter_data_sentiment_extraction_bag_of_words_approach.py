# -*- coding: utf-8 -*-
"""Twitter Data Sentiment Extraction - Bag of Words Approach.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KgoaNDQaN0VimnMEQqXJrPPYkJAXAkKB
"""

import numpy as np
import pandas as pd
import nltk #natural language tool-kit

import os
for dirname, _, filenames in os.walk("D:/Research/"):
    for filename in filenames:
        print(os.path.join(dirname, filename))

def jaccard(txt1, txt2):
    str1, str2 = str(txt1), str(txt2)
    a = set(str1.lower().split())
    b = set(str2.lower().split())
    c = a.intersection(b)# J(A,B) = |A∩B| / |A∪B| = |{0,2,5}| / |{0,1,2,3,4,5,6,7,9}| = 3/9 = 0.33
    return float(len(c))/(len(a)+len(b)-len(c))

train = pd.read_csv("D:/Research/train.csv")
test = pd.read_csv("D:/Research/test.csv")
sample_submission = pd.read_csv("D:/Research/sample_submission.csv")

train.head()

v1 = train.loc[train.sentiment == 'neutral', 'text'].values.tolist()#from object to array to list
v2 = train.loc[train.sentiment == 'neutral', 'selected_text'].values.tolist()
np.mean([jaccard(w1, w2) for w1, w2 in zip(v1, v2)])

test.head()

sample_submission.head()

isNeutral = test.loc[test.sentiment == 'neutral', 'textID'].values.tolist()
isNeutral

def get_selected_text_neutral(textID, df=test):
    if textID in isNeutral:
        return df.loc[df.textID == textID, 'text'].values.tolist()
    else:
        return np.nan
def treat_neutral(sample_submission):
    sample_submission['selected_text'] = sample_submission['textID'].apply(get_selected_text_neutral, df=test)
    return sample_submission
sample_submission = treat_neutral(sample_submission)
sample_submission

"""I'll treat the positive and negative tweets with the help of nltk now."""

from nltk import pos_tag, ngrams
#nltk.download('sentiwordnet')
from nltk.corpus import sentiwordnet as swn, wordnet as wn
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer
lemmatizer = WordNetLemmatizer()
ps = PorterStemmer()

import string 
filter_word = [wn.NOUN, wn.ADJ, wn.ADV, wn.VERB]
filter_word

"""PenTreeBank 

Tags, Descriptions


1.	JJ	Adjective
2.	JJR	Adjective, comparative
3.	JJS	Adjective, superlative


4.	NN	Noun, singular or mass
5.	NNS	Noun, plural
6.	NNP	Proper noun, singular
7.	NNPS	Proper noun, plural


8.	RB	Adverb
9.	RBR	Adverb, comparative
10.	RBS	Adverb, superlative
11.	RP	Particle


12.	VB	Verb, base form
13.	VBD	Verb, past tense
14.	VBG	Verb, gerund or present participle
15.	VBN	Verb, past participle
16.	VBP	Verb, non-3rd person singular present
17.	VBZ	Verb, 3rd person singular present
"""

def penn_to_wn(tag):
    '''
    Convert PennTreeBank tags to simple wordnet tags
    '''
    if tag.startswith('J'):
        return wn.ADJ
    if tag.startswith('N'):
        return wn.NOUN
    if tag.startswith('V'):
        return wn.VERB
    if tag.startswith('R'):
        return wn.ADV
    else:
        return None

def get_sentiment(word, tag, verbose = 0):
    '''
    returns a list of pos, neg and objective score. But returns empty list if not present in senti wordnet. 
    '''
    wn_tag = penn_to_wn(tag)
    if wn_tag not in filter_word:
        return []
    
    lemma = lemmatizer.lemmatize(word, pos = wn_tag)
    if verbose:
        print(f'Lemmatizer: {lemma}')
    if not lemma:
        return []
    
    synsets = wn.synsets(word, pos = wn_tag)
    if verbose:
        print(f'Synsets: {synsets}')
    if not synsets:
        return []
    
    swn_synset_pos = []
    swn_synset_neg = []
    for synset in synsets:
        swn_synset = swn.senti_synset(synset.name())
        if verbose:
            print(f'Pos Score: {swn_synset.pos_score()}, Neg Score: {swn_synset.neg_score()}')
        swn_synset_pos.append(swn_synset.pos_score())
        swn_synset.neg.append(swn_synset.neg_score())
    return [np.mean(swn_synset_pos), np.mean(swn_synset_neg)]#swn_synset.obj_score
        
def robustify(text = ''):
    if type(text) != str:
        try:
            text = str(text)
        except:
            text = ''
    return text

def score(text = '', verbose = 0):
    text = robustify(text)
    for dot in string.punctuation:
        text = text.replace(dot,'')
    tokenize_text = word_tokenize(text)
    if verbose:
        print(f'Tokenized text: {tokenize_text}')
    tags = pos_tag(tokenize_text)
    senti_val = [(x.lower(), get_sentiment(x.lower(), y, verbose)) for (x,y) in tags]
    senti_val = list(filter(lambda x: len(x[1]) > 0, senti_val))
    return senti_val

for synset in wn.synsets('great', 'a'):
    print(swn.senti_synset(synset.name()).obj_score())

def get_sentiment(word, tag, verbose = 0):
    '''
    returns a list of pos, neg and objective score. But returns empty list if not present in senti wordnet. 
    '''
    wn_tag = penn_to_wn(tag)
    if wn_tag not in filter_word:
        return []
    
    lemma = lemmatizer.lemmatize(word, pos = wn_tag)
    if verbose:
        print(f'Lemmatizer: {lemma}')
    if not lemma:
        return []
    
    synsets = wn.synsets(word, pos = wn_tag)
    if verbose:
        print(f'Synsets: {synsets}')
    if not synsets:
        return []
    
    swn_synset_pos = []
    swn_synset_neg = []
    for synset in synsets:
        swn_synset = swn.senti_synset(synset.name())
        if verbose:
            print(f'Pos Score: {swn_synset.pos_score()}, Neg Score: {swn_synset.neg_score()}')
        swn_synset_pos.append(swn_synset.pos_score())
        swn_synset_neg.append(swn_synset.neg_score())
    return [np.mean(swn_synset_pos), np.mean(swn_synset_neg)]#swn_synset.obj_score

def robustify(text = ''):
    if type(text) != str:
        try:
            text = str(text)
        except:
            text = ''
    return text
text = robustify("Hello, how are you?")
text

for dot in string.punctuation:
    text = text.replace(dot,'')
text

tokenize_text = word_tokenize(text)
tokenize_text

tags = pos_tag(tokenize_text)
tags

senti_val = [(x.lower(), get_sentiment(x.lower(), y, verbose=0)) for (x,y) in tags]

for x in senti_val:
    print(x[1], len(x[1]))

score('happy bday!', verbose=1)

lemmatizer.lemmatize("happy", pos = 'n')

test['senti_val'] = test['text'].apply(score)

test.loc[test.sentiment != 'neutral'].head()

def treat_senti_val(sentiment, senti_val):
    if sentiment == 'neutral':
        return []
    sent = 0 if sentiment == 'positive' else 1
    return [(t[0], t[1][sent]) for t in senti_val]
test['senti_val'] = test.apply(lambda df: treat_senti_val(df.sentiment, df.senti_val), axis=1)
#test.head(20)

def get_selected_text(text, senti_val):
    if senti_val == 0:
        return text
    else:
        return ''.join([t[0] for t in senti_val if t[1] > 0.1])
test['selected_text'] = test.apply(lambda df: get_selected_text(df.text, df.senti_val), axis = 1)
test.head()

for i, row in test.loc[test.sentiment != 'neutral'].iterrows():
    sample_submission.loc[sample_submission.textID == row['textID'], 'selected_text'] = row['selected_text']

sample_submission.head()